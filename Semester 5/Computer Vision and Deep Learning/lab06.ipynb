{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hXhoiRUvnHJ"
   },
   "source": [
    "# Computer vision and deep learning - Laboratory 6\n",
    "\n",
    "In this last laboratory, we will switch our focus from implementing and training neural networks to developing a machine learning application.\n",
    "More specifically you will learn how you can convert your saved torch model into a more portable format using torch script and how you can create a simple demo application for your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_8pVcLUMVQB",
    "outputId": "c48b4fe3-b4a6-40ad-f3db-cd2085654e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in e:\\anaconda3\\lib\\site-packages (4.14.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in e:\\anaconda3\\lib\\site-packages (from gradio) (0.109.0)\n",
      "Requirement already satisfied: ffmpy in e:\\anaconda3\\lib\\site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.8.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (0.8.0)\n",
      "Requirement already satisfied: httpx in e:\\anaconda3\\lib\\site-packages (from gradio) (0.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in e:\\anaconda3\\lib\\site-packages (from gradio) (0.20.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in e:\\anaconda3\\lib\\site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: numpy~=1.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (1.24.3)\n",
      "Requirement already satisfied: orjson~=3.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in e:\\anaconda3\\lib\\site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (2.5.3)\n",
      "Requirement already satisfied: pydub in e:\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in e:\\anaconda3\\lib\\site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in e:\\anaconda3\\lib\\site-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in e:\\anaconda3\\lib\\site-packages (from gradio) (0.26.0)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda3\\lib\\site-packages (from gradio-client==0.8.0->gradio) (2023.12.2)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in e:\\anaconda3\\lib\\site-packages (from gradio-client==0.8.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in e:\\anaconda3\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in e:\\anaconda3\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.9.0)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in e:\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in e:\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\anaconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (8.0.4)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in e:\\anaconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in e:\\anaconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in e:\\anaconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in e:\\anaconda3\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in e:\\anaconda3\\lib\\site-packages (from fastapi->gradio) (0.35.1)\n",
      "Requirement already satisfied: anyio in e:\\anaconda3\\lib\\site-packages (from httpx->gradio) (3.5.0)\n",
      "Requirement already satisfied: certifi in e:\\anaconda3\\lib\\site-packages (from httpx->gradio) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda3\\lib\\site-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: idna in e:\\anaconda3\\lib\\site-packages (from httpx->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in e:\\anaconda3\\lib\\site-packages (from httpx->gradio) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in e:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in e:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.16)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n",
      "Requirement already satisfied: torch in e:\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in e:\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in e:\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda3\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in e:\\anaconda3\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.2 in e:\\anaconda3\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in e:\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in e:\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in e:\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in e:\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-btbzsULYXM"
   },
   "source": [
    "# Creating a simple UI with gradio\n",
    "\n",
    "\n",
    "[Gradio](https://www.gradio.app/docs/interface) is an open-source Python library used for creating customizable UI components for machine learning models with just a few lines of code. It greatly simplifies the process of building web-based interfaces to interact with ML models without requiring extensive knowledge of web development and allows you to quickly build an MVP and get feedback from the users.\n",
    "\n",
    "\n",
    "To get an application running, you just need to specify three parameters:\n",
    "1. the function to wrap the interface around.\n",
    "2. what are the desired input components?\n",
    "3. what are the desired output components?\n",
    "\n",
    "\n",
    "This is achieved through the ``gradio.Interface`` class, the central component in gradio, responsible for creating the user interface for your machine learning model.\n",
    "\n",
    "\n",
    "```\n",
    "import gradio as gr\n",
    "demo = gr.Interface(fn=image_classifier,\n",
    "                    inputs=\"image\",\n",
    "                    outputs=\"label\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Once you've defined the gr.Interface, the launch() method is used to start the interface, making it accessible through a web browser.\n",
    "\n",
    "\n",
    "```\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "\n",
    "When the launch method is called, ```gradio``` launches a simple web server that serves the demo. If you specify ```share=True``` when calling the launch function, ```gradio``` will create a public link Can also be used to create a public link used by anyone to access the demo from their browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXflaukmmWPh"
   },
   "source": [
    "## Simple UI for image classification in gradio\n",
    "\n",
    "Below you have an example of how you could use ```gradio``` to create a simple UI for an image classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "fpXMo37vMUYL",
    "outputId": "99a5b2a7-9173-42fc-96e7-ba2aba458039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7896\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7896/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def softmax(x):\n",
    "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "\n",
    "\n",
    "def classify_image(img):\n",
    "    # TODO run a classification model to get the class scores\n",
    "    prediction = softmax(np.random.randn(10, ))\n",
    "    confidences = {CLASSES[i]: float(prediction[i]) for i in range(len(CLASSES))}\n",
    "    return confidences\n",
    "\n",
    "ui = gr.Interface(fn=classify_image,\n",
    "             inputs=gr.Image(),\n",
    "             outputs=gr.Label(num_top_classes=3),\n",
    "             # TODO replace example1.png example2.png with some images from your device\n",
    "            #examples=['example1.png', 'example2.png']\n",
    "          )\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKXtFC0_hkcm"
   },
   "source": [
    "## Accessing the webcam with gradio\n",
    "\n",
    "In the example below, you have an example in which you take the input images from your webcam.\n",
    "The function wrapped by gradio uses a mask to blur the input image outside that mask. If you plan to do background blurring, the mask could be the segmentation mask predicted by your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "HzC5frqLhk1I",
    "outputId": "c1e40a6a-c49b-4a93-f914-fce4eeb17341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7897\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7897/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "def blur_background(input_image):\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Generate a blank mask\n",
    "    # TODO your code here: call a segmentation model to get predicted mask\n",
    "    mask = np.zeros_like(input_image)\n",
    "\n",
    "    # for demo purposes, we are going to create a random segmentation mask\n",
    "    #  just a circular blob centered in the middle of the image\n",
    "    center_x, center_y = mask.shape[1] // 2, mask.shape[0] // 2\n",
    "    cv2.circle(mask, (center_x, center_y), 100, (255, 255, 255), -1)\n",
    "\n",
    "    # Convert the mask to grayscale\n",
    "    mask_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    mask_gray = mask_gray[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "    # apply a strong Gaussian blur to the areas outside the mask\n",
    "    blurred = cv2.GaussianBlur(input_image, (51, 51), 0)\n",
    "    result = np.where(mask_gray, input_image, blurred)\n",
    "\n",
    "    # Convert the result back to RGB format for Gradio\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "    return result\n",
    "\n",
    "\n",
    "ui = gr.Interface(\n",
    "    fn=blur_background,\n",
    "    inputs=gr.Image(sources=[\"webcam\"]),\n",
    "    outputs=\"image\",\n",
    "    title=\"Image segmentation demo!\"\n",
    "\n",
    ")\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhaGtNlJj1Wr"
   },
   "source": [
    "## Laboratory assignment\n",
    "\n",
    "\n",
    "Now you have all the knowledge required to build your own ML semantic segmentation application.\n",
    "\n",
    "\n",
    "1. First use ```torchscript``` to obtain a model binary.\n",
    "2. Using gradio, create a simple application that uses the semantic segmentation that you developed. Feel free to define the scope and the functional requirements of your app.\n",
    "3. __[Optional, independent work]__ Use a serverless cloud function on [AWS Lambda](https://aws.amazon.com/lambda/) (this requires an account on Amazon AWS and you need to provide the details of a credit card) to run the prediction and get the results.\n",
    "\n",
    "\n",
    "Congratulations, you've just completed all the practical work for Computer Vision and Deep Learning!\n",
    "May your data always be clean, your models accurate, and your code bug-free!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def center_crop(original_image, target_image):\n",
    "    # get dimensions of the original and target images\n",
    "    original_height, original_width = original_image.shape[-2], original_image.shape[-1]\n",
    "    target_height, target_width = target_image.shape[-2], target_image.shape[-1]\n",
    "\n",
    "    # calculate starting indices for cropping\n",
    "    start_height = max(0, (original_height - target_height) // 2)\n",
    "    start_width = max(0, (original_width - target_width) // 2)\n",
    "\n",
    "    # calculate ending indices for cropping\n",
    "    end_height = start_height + target_height\n",
    "    end_width = start_width + target_width\n",
    "\n",
    "    # perform cropping\n",
    "    cropped_image = original_image[..., start_height:end_height, start_width:end_width]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "class DoubleSamplingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        # sequential block with two convolutional layers\n",
    "        self.double_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(mid_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.double_conv(X)\n",
    "    \n",
    "\n",
    "class EncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            # 2x2 max pooling at the beginning\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            # 2 upsample blocks\n",
    "            DoubleSamplingBlock(in_channels, out_channels=out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.encoder(X)\n",
    "\n",
    "\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(\n",
    "                in_channels=input_channels,\n",
    "                out_channels=input_channels // 2,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(input_channels // 2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.sampling_block = DoubleSamplingBlock(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=output_channels,\n",
    "            mid_channels=input_channels // 2\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_features, X):\n",
    "        X = self.up(X)\n",
    "\n",
    "        # center crop the encoder features to match the input\n",
    "        encoder_features = center_crop(encoder_features, X)\n",
    "\n",
    "        # concatenate the encoder features and input along the channel dimension\n",
    "        X = torch.cat([encoder_features, X], dim=1)\n",
    "\n",
    "        # apply the sampling block\n",
    "        X = self.sampling_block(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, intermediary_filters=64, num_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # initial convolution block\n",
    "        self.in_convolution = DoubleSamplingBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=intermediary_filters,\n",
    "            mid_channels=64\n",
    "        )\n",
    "\n",
    "        # list of encoder blocks\n",
    "        self.encoders = torch.nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                in_channels=intermediary_filters * 2**i,\n",
    "                out_channels=2 * intermediary_filters * 2**i\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # list of decoder blocks\n",
    "        self.decoders = torch.nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                input_channels=2 * intermediary_filters * 2**i,\n",
    "                output_channels=intermediary_filters * 2**i\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # output convolution layer\n",
    "        self.out_convolution = torch.nn.Conv2d(\n",
    "            in_channels=intermediary_filters,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.in_convolution(X)\n",
    "\n",
    "        # list to store intermediate outputs from each encoder block\n",
    "        outputs = [X]\n",
    "\n",
    "        # forward pass through encoder blocks\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            outputs.append(encoder.forward(outputs[i]))\n",
    "\n",
    "        # get the output from the last encoder block\n",
    "        X = outputs[-1]\n",
    "\n",
    "        # forward pass through decoder blocks in reverse order\n",
    "        for i, decoder in enumerate(self.decoders[::-1]):\n",
    "            X = decoder.forward(outputs[len(self.decoders) - 1 - i], X)\n",
    "\n",
    "        # apply the final output convolution layer\n",
    "        X = self.out_convolution(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Downloads/saved_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as v2\n",
    "\n",
    "INPUT_SHAPE = (128, 128)\n",
    "\n",
    "def custom_transform(X):\n",
    "    with torch.no_grad():\n",
    "        if X is not None:\n",
    "            # transpose channels to PyTorch format (C, H, W)\n",
    "            X = cv2.cvtColor(X, cv2.COLOR_BGR2RGB)\n",
    "            X = X.transpose([2, 0, 1])\n",
    "            # convert to PyTorch tensor\n",
    "            X = torch.from_numpy(X)\n",
    "            X = X.to(torch.float32)\n",
    "            # normalize pixel values to the range [0, 1]\n",
    "            X = X / 255\n",
    "            # resize image to the desired input shape\n",
    "            X = torch.nn.functional.interpolate(X.view(-1, *X.shape), size=INPUT_SHAPE).view(-1, *INPUT_SHAPE)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def apply_inverse_transform(image, label):\n",
    "    with torch.no_grad():\n",
    "        if label is not None:\n",
    "            # convert label to one-hot encoding and scale\n",
    "            label = torch.nn.functional.one_hot(label, num_classes=3) * torch.tensor(255)\n",
    "            label = label.to(torch.uint8)\n",
    "            label = label.cpu().numpy()\n",
    "\n",
    "        if image is not None:\n",
    "            # scale image values and transpose\n",
    "            image = image.squeeze().permute(1, 2, 0)\n",
    "            image = (image * 255).to(torch.uint8)\n",
    "            image = cv2.cvtColor(image.cpu().numpy(), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def predict_image(input_image):\n",
    "    image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "    initial_image_size = image.shape[:-1]\n",
    "\n",
    "    image = custom_transform(image)\n",
    "    image = image.unsqueeze(dim=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image = image.to(device)\n",
    "\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_labels = loaded_model(image)\n",
    "\n",
    "    predicted_labels = torch.nn.functional.interpolate(predicted_labels, size=tuple(INPUT_SHAPE))\n",
    "    predicted_labels = predicted_labels.squeeze(dim=0).argmax(dim=0)\n",
    "\n",
    "    image, predicted_labels = apply_inverse_transform(image, predicted_labels)\n",
    "\n",
    "    predicted_labels = cv2.resize(predicted_labels, initial_image_size)\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7919\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7919/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_image,     \n",
    "    inputs=gr.Image(sources=[\"upload\"]),\n",
    "    outputs=[\"image\"],\n",
    "    live=True,\n",
    "    title=\"Image segmentation\"\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "\n",
    "# it has pretty bad results because I used a poorly trained version of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
